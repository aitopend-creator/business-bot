<script type="module">
  import { CreateWebWorkerMLCEngine } from "https://esm.sh/@mlc-ai/web-llm";

  const statusEl = document.getElementById('status');
  const sendBtn  = document.getElementById('send');
  const promptEl = document.getElementById('prompt');
  const threadEl = document.getElementById('thread');

  // 0) Browser check
  if (!('gpu' in navigator)) {
    statusEl.textContent = "Your browser doesn't have WebGPU. Use Chrome/Edge (latest).";
  }

  // 1) Choose a smaller, faster model
  // Good starter that loads quicker on most laptops/phones:
  const model = "Phi-3-mini-4k-instruct-q4f16_1-MLC";

  // 2) Show load progress
  function progress(p) {
    const pct = Math.round((p.progress || 0) * 100);
    statusEl.textContent = p.text ? `${p.text} (${pct}%)` : `Loading model… (${pct}%)`;
  }

  let engine;
  try {
    engine = await CreateWebWorkerMLCEngine(
      { model },
      {
        initProgressCallback: progress,
        worker: new Worker("https://esm.sh/@mlc-ai/web-llm/dist/worker.js", { type: "module" })
      }
    );
    statusEl.textContent = "AI ready. Ask me anything!";
  } catch (e) {
    console.error(e);
    statusEl.textContent = "Failed to load AI. Try Chrome/Edge (latest), then hard refresh.";
  }

  // Conversation with a system prompt that pushes to book
  const messages = [{
    role: "system",
    content:
      "You are a friendly website receptionist. Always: 1) Answer briefly, " +
      "2) Ask one qualifying follow-up, 3) Offer the 'Book now' button on the page, " +
      "4) Be cautious with specifics; invite them to book for details."
  }];

  function addMsg(role, text) {
    const div = document.createElement('div');
    div.className = 'row';
    const bubble = document.createElement('div');
    bubble.className = 'msg ' + (role === 'user' ? 'you' : 'bot');
    bubble.textContent = text;
    div.appendChild(bubble);
    threadEl.appendChild(div);
    threadEl.scrollTop = threadEl.scrollHeight;
  }

  addMsg('assistant', 'Welcome! How can I help you today?');

  async function ask() {
    const userText = promptEl.value.trim();
    if (!userText || !engine) return;
    promptEl.value = "";
    addMsg('user', userText);

    messages.push({ role: "user", content: userText });
    sendBtn.disabled = true; statusEl.textContent = "Thinking…";

    try {
      let reply = "";
      const stream = await engine.chat.completions.create({
        stream: true,
        messages
      });

      // live stream
      let temp;
      for await (const chunk of stream) {
        const delta = chunk.choices?.[0]?.delta?.content || "";
        if (!delta) continue;
        reply += delta;
        if (!temp) {
          temp = document.createElement('div');
          temp.className = 'row';
          const bubble = document.createElement('div');
          bubble.className = 'msg bot';
          bubble.textContent = '';
          temp.appendChild(bubble);
          threadEl.appendChild(temp);
        }
        temp.querySelector('.msg').textContent = reply;
        threadEl.scrollTop = threadEl.scrollHeight;
      }

      if (temp) temp.remove();
      addMsg('assistant', reply || "(no text)");
      messages.push({ role: "assistant", content: reply || "" });
      statusEl.textContent = "Ready.";
    } catch (err) {
      console.error(err);
      statusEl.textContent = "Error generating reply. Try again.";
      addMsg('assistant', "Sorry—had a hiccup. Please ask again!");
    } finally {
      sendBtn.disabled = false;
    }
  }

  sendBtn.onclick = ask;
  promptEl.addEventListener('keydown', (e) => {
    if (e.key === 'Enter' && !e.shiftKey) { e.preventDefault(); ask(); }
  });
</script>
